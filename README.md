# Algorithm Visualization Platform

## Project Overview

An educational platform for visualizing algorithms with active learning features. Built on a **registry-based architecture** that makes adding new algorithms as simple as registering a class‚Äîno endpoint configuration required.

**Philosophy:** Backend does ALL the thinking, frontend does ALL the reacting.

**Status:** ‚úÖ Platform Architecture Complete - 2 Algorithms Live (Interval Coverage, Binary Search)

---

## üéØ Core Architecture Principles

### The Registry Pattern (‚≠ê Core Innovation)

**Critical Rule:** You do NOT modify `app.py` routing logic. Algorithms self-register and appear in the UI automatically.

**How It Works:**

```python
# backend/algorithms/your_algorithm.py
class YourAlgorithmTracer(AlgorithmTracer):
    def execute(self, input_data):
        # Your algorithm + trace generation
        return self._build_trace_result(result)

    def get_prediction_points(self):
        # Identify learning moments
        return [...]

    def generate_narrative(self, trace_result):
        # Convert trace to human-readable markdown (REQUIRED v2.0+)
        return "# Algorithm Execution\n\n..."

# backend/algorithms/registry.py
registry.register(
    name='your-algorithm',
    tracer_class=YourAlgorithmTracer,
    display_name='Your Algorithm',
    description='What it does',
    example_inputs=[...]
)
```

**Result:** Algorithm automatically appears in UI dropdown. No app.py changes. No frontend routing changes. ‚ú®

---

### Unified API Endpoint

**Single endpoint handles ALL algorithms:**

```bash
POST /api/trace/unified
{
  "algorithm": "binary-search",  # or "interval-coverage", "merge-sort", etc.
  "input": {
    "array": [1, 3, 5, 7, 9],
    "target": 5
  }
}
```

**Backend Routing:**

```python
# app.py - This handles ALL algorithms automatically
@app.route('/api/trace/unified', methods=['POST'])
def generate_trace_unified():
    algorithm_name = request.json['algorithm']
    algorithm_input = request.json['input']

    # Registry lookup (automatic)
    tracer_class = registry.get(algorithm_name)
    tracer = tracer_class()

    # Execute and return trace
    return jsonify(tracer.execute(algorithm_input))
```

---

### Dynamic Component Selection

**Backend declares visualization type; frontend selects components automatically for both panels:**

**LEFT Panel (Visualization):** Registry selects visualization component based on `visualization_type`

**RIGHT Panel (Algorithm State):** Registry selects state component based on `algorithm` name

```python
# Backend declares visualization type
self.metadata = {
    'algorithm': 'binary-search',
    'visualization_type': 'array',  # ‚Üê Frontend LEFT panel reads this
    'visualization_config': {...}
}
```

```javascript
// Frontend LEFT panel - dynamically selects visualization
import { getVisualizationComponent } from "./utils/visualizationRegistry";

const VisualizationComponent = getVisualizationComponent(
  trace.metadata.visualization_type // 'array' ‚Üí ArrayView
);

// Frontend RIGHT panel - dynamically selects state component
import { getStateComponent } from "./utils/stateRegistry";

const StateComponent = getStateComponent(
  currentAlgorithm // 'binary-search' ‚Üí BinarySearchState
);
```

**Available Visualization Types:**

- `array` - For Binary Search, Sorting algorithms
- `timeline` - For Interval Coverage
- `graph` - Future: DFS, BFS, Dijkstra
- `tree` - Future: BST, Heap operations

---

## Project Structure

```
interval-viz-poc/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ algorithms/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_tracer.py          # ‚≠ê Abstract base class (CRITICAL)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ registry.py             # ‚≠ê Central algorithm registry
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ interval_coverage.py    # Example algorithm
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ binary_search.py        # Example algorithm
‚îÇ   ‚îú‚îÄ‚îÄ app.py                      # Flask API with unified routing
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îÇ
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AlgorithmSwitcher.jsx    # Dynamic algorithm selector
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ControlBar.jsx           # Navigation controls
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CompletionModal.jsx      # Success screen
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PredictionModal.jsx      # Interactive predictions
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ KeyboardHints.jsx        # Shortcut guide
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ panels/                  # ‚≠ê Layout Containers
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ VisualizationPanel.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ StatePanel.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ algorithm-states/        # ‚≠ê Algorithm-specific state components
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ BinarySearchState.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ IntervalCoverageState.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ visualizations/          # ‚≠ê Reusable viz components
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ ArrayView.jsx
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ TimelineView.jsx
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ index.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ contexts/                    # ‚≠ê State Management (Context API)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TraceContext.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ NavigationContext.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PredictionContext.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ KeyboardContext.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hooks/                       # Context Consumers
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useTraceLoader.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useTraceNavigation.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ usePredictionMode.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useVisualHighlight.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ useKeyboardShortcuts.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stateRegistry.js         # ‚≠ê Dynamic state component selection
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ visualizationRegistry.js # ‚≠ê Dynamic visualization selection
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.jsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index.js
‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ compliance/                      # ‚≠ê Compliance checklists & workflow
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ WORKFLOW.md                  # ‚≠ê Single source of truth
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ BACKEND_CHECKLIST.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FAA_PERSONA.md               # ‚≠ê Arithmetic audit guide (v2.1)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FRONTEND_CHECKLIST.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ QA_INTEGRATION_CHECKLIST.md
‚îÇ   ‚îî‚îÄ‚îÄ ADR/                             # Architecture decision records
‚îÇ       ‚îú‚îÄ‚îÄ ADR-001-registry-based-architecture.md
‚îÇ       ‚îî‚îÄ‚îÄ ADR-002-component-organization-principles.md
‚îÇ
‚îî‚îÄ‚îÄ README.md

‚≠ê = Critical files for understanding the architecture
```

---

## Compliance Framework (CRITICAL)

This platform follows a **three-tier requirement system** that defines what can and cannot be changed.

### Requirement Tiers

#### 1. LOCKED Requirements üîí

**Cannot be changed without major version bump**

- **API Contracts**: Trace structure, metadata fields
- **Modal Behavior**: HTML IDs (`#prediction-modal`, `#completion-modal`), keyboard shortcuts, auto-scroll
- **Panel Layout**: Overflow pattern (MUST use `items-start` + `mx-auto`, NOT `items-center`)
- **Narrative Generation**: All algorithms MUST implement `generate_narrative()` (v2.0+)

#### 2. CONSTRAINED Requirements ‚ö†Ô∏è

**Limited flexibility with defined bounds**

- **Visualization Data**: Array/timeline/graph patterns
- **Prediction Format**: ‚â§3 choices maximum
- **Step Type Categorization**: 7 defined types (DECISION, COVERAGE, etc.)

#### 3. FREE Zones ‚úÖ

**Full creative freedom**

- Internal algorithm implementation
- Performance optimizations
- Custom visualization styles (within overflow pattern)

---

### Four Compliance Stages (CRITICAL)

All new algorithms MUST pass these stages:

#### Stage 1: Backend Implementation & Checklist (`docs/compliance/BACKEND_CHECKLIST.md`)

**Validates:**

- ‚úÖ Metadata structure (`algorithm`, `display_name`, `visualization_type`)
- ‚úÖ Trace format (steps array, timestamps, descriptions)
- ‚úÖ Visualization data contracts (use `state` string, not `visual_state` dict)
- ‚úÖ Prediction points format (‚â§3 choices)
- ‚úÖ Base class compliance (`AlgorithmTracer` inheritance)
- ‚úÖ **Narrative generation implemented** (v2.0+)

**Critical Anti-Patterns:**

- ‚ùå Missing `display_name` field
- ‚ùå Using `visual_state` dict instead of `state` string
- ‚ùå >3 choices in prediction questions
- ‚ùå Hardcoding visualization logic in tracer
- ‚ùå Missing `generate_narrative()` implementation

---

#### Stage 1.5: FAA Audit (`docs/compliance/FAA_PERSONA.md`) - NEW in v2.1

**Validates:**

- ‚úÖ Arithmetic correctness of all quantitative claims
- ‚úÖ State transition mathematics (e.g., "updated from X ‚Üí Y")
- ‚úÖ Visualization-text alignment (counts match what's shown)
- ‚úÖ No copy-paste errors or stale state propagation

**Critical:** This is a **BLOCKING gate**. Narratives with arithmetic errors cannot proceed to QA review. Catches math bugs in 10-15 minutes vs. 2 days of integration debugging.

**FAA ONLY validates mathematics, NOT:**

- ‚ùå Pedagogical quality (QA handles this in Stage 2)
- ‚ùå Narrative completeness (QA handles this in Stage 2)
- ‚ùå Writing style or clarity (QA handles this in Stage 2)

**Common errors caught:**

- Copy-paste errors (same number after different operations)
- Stale state propagation (previous step's value incorrectly carried forward)
- Off-by-one errors in index arithmetic
- Visualization-text mismatches

---

#### Stage 2: QA Narrative Review

**Validates:**

- ‚úÖ Logical completeness (can follow algorithm from narrative alone)
- ‚úÖ Temporal coherence (step N ‚Üí N+1 makes sense)
- ‚úÖ Decision transparency (all comparison data visible)
- ‚ö†Ô∏è **Assumes arithmetic already verified by FAA**

**QA does NOT validate:**

- ‚ùå Arithmetic correctness (FAA already handled)
- ‚ùå Whether JSON structure is correct (Backend Checklist)
- ‚ùå Whether frontend can render it (Integration Tests)

---

#### Stage 3: Frontend Integration (`docs/compliance/FRONTEND_CHECKLIST.md`)

**Validates:**

- ‚úÖ Modal IDs: `#prediction-modal`, `#completion-modal` (LOCKED)
- ‚úÖ Overflow pattern: `items-start` + `mx-auto` (NOT `items-center`)
- ‚úÖ Keyboard shortcuts (‚Üê‚Üí navigation, R reset, K/C/S prediction)
- ‚úÖ Auto-scroll behavior in call stack
- ‚úÖ Component interface (`step` and `config` props)

**Critical Overflow Pattern:**

```javascript
// ‚úÖ CORRECT: Prevents left-side cutoff
<div className="h-full flex flex-col items-start overflow-auto py-4 px-6">
  <div className="mx-auto">
    {/* content centers but doesn't cut off */}
  </div>
</div>

// ‚ùå INCORRECT: Causes overflow cutoff on left side
<div className="h-full flex flex-col items-center overflow-auto">
  {/* content gets cut off */}
</div>
```

---

#### Stage 4: Integration Testing (`docs/compliance/QA_INTEGRATION_CHECKLIST.md`)

**14 Test Suites:**

- Suite 1-6: LOCKED requirements (modals, IDs, keyboard, auto-scroll, overflow)
- Suite 7-10: CONSTRAINED requirements (backend contract, predictions)
- Suite 11-14: Integration tests (cross-algorithm, responsive, performance, regression)

**Expected Outcome (v2.1):**

- Zero "missing data" bugs (narrative review caught them)
- Zero "arithmetic error" bugs (FAA caught them)

---

**Complete Workflow:**

```
Backend Implementation
    ‚Üì
Generate Narratives
    ‚Üì
FAA Arithmetic Audit (BLOCKING) ‚Üê NEW in v2.1
    ‚Üì
Backend Checklist
    ‚Üì
QA Narrative Review (assumes math verified)
    ‚Üì
Frontend Integration
    ‚Üì
Frontend Checklist
    ‚Üì
Integration Tests
    ‚Üì
Production ‚úÖ
```

---

## Base Tracer Abstraction (CRITICAL)

All algorithms MUST inherit from `AlgorithmTracer`:

```python
class AlgorithmTracer(ABC):
    @abstractmethod
    def execute(self, input_data: Any) -> dict:
        """
        Execute algorithm and return standardized result.

        REQUIRED FIELDS in metadata:
        - display_name: str (UI display name)
        - visualization_type: str ('array', 'timeline', 'graph', 'tree')

        Returns:
        {
            "result": <algorithm output>,
            "trace": {"steps": [...], "total_steps": N, "duration": T},
            "metadata": {
                "algorithm": "name",
                "display_name": "Display Name",      # REQUIRED
                "visualization_type": "array",       # REQUIRED
                "visualization_config": {...},
                "prediction_points": [...]           # Auto-generated
            }
        }
        """
        pass

    @abstractmethod
    def get_prediction_points(self) -> List[Dict[str, Any]]:
        """
        Identify prediction moments in the trace for active learning.

        Returns a list of prediction opportunities where students should
        pause and predict the algorithm's next decision.

        CRITICAL: Maximum 3 choices per question.

        Returns: [
            {
                "step_index": int,           # Which step to pause at
                "question": str,             # Question to ask student
                "choices": [str, ...],       # Possible answers (‚â§3)
                "hint": str,                 # Optional hint
                "correct_answer": str        # For validation
            }
        ]

        Example for interval coverage:
            {
                "step_index": 5,
                "question": "Will this interval be kept or covered?",
                "choices": ["keep", "covered"],
                "hint": "Compare interval.end with max_end",
                "correct_answer": "keep"
            }

        Example for binary search:
            {
                "step_index": 3,
                "question": "Will we search left or right of mid?",
                "choices": ["search-left", "search-right", "found"],
                "hint": "Compare mid value with target",
                "correct_answer": "search-right"
            }
        """
        pass

    @abstractmethod
    def generate_narrative(self, trace_result: dict) -> str:
        """
        Convert trace JSON to human-readable markdown narrative.

        REQUIRED since v2.0 (WORKFLOW.md). This narrative is reviewed by QA
        BEFORE frontend integration to catch missing data early.

        CRITICAL REQUIREMENTS:
        1. Show ALL decision data - if you reference a variable, SHOW its value
        2. Make comparisons explicit with actual values
        3. Explain decision outcomes clearly
        4. Fail loudly (KeyError) if visualization data is incomplete
        5. Narrative must be self-contained and logically complete

        Args:
            trace_result: Complete trace dictionary from execute()
                         Contains: result, trace, metadata

        Returns:
            str: Markdown-formatted narrative showing all decision logic
                 with supporting data visible at each step

        Raises:
            KeyError: If visualization data incomplete (fail loudly - catches bugs!)

        Example Structure:
            # [Algorithm Name] Execution Narrative

            **Input:** [Describe input with key parameters]
            **Goal:** [What we're trying to achieve]

            ## Step 0: [Description]
            **State:** [Show relevant visualization state]
            **Decision:** [If applicable, show comparison with actual values]
            **Result:** [Outcome of decision]

            ## Step 1: ...

            ## Final Result
            **Output:** [Algorithm result]
            **Performance:** [Key metrics if applicable]

        Good Patterns:
        - ‚úÖ "Compare interval.start (600) with max_end (660) ‚Üí 600 < 660"
        - ‚úÖ "Decision: Keep interval [600, 720] because it extends coverage"
        - ‚úÖ Show array/graph state at each decision point
        - ‚úÖ Temporal coherence: step N clearly leads to step N+1

        Anti-Patterns to AVOID:
        - ‚ùå Referencing undefined variables: "Compare with max_end" (but max_end not shown)
        - ‚ùå Skipping decision outcomes: "Examining interval... [next step unrelated]"
        - ‚ùå Narratives requiring code to understand
        """
        pass
```

**Built-in Methods:**

- `_add_step(type, data, description)` - Record trace steps
- `_build_trace_result(result)` - Format standardized output
- `_get_visualization_state()` - Optional: Auto-enrich steps

**Safety Limits:**

- `MAX_STEPS = 10,000` - Prevents infinite loops
- Automatic error handling

---

## API Documentation (CRITICAL)

### Primary Endpoints

#### `GET /api/algorithms`

**Purpose:** Discover all available algorithms with metadata.

**Response:**

```json
[
  {
    "name": "binary-search",
    "display_name": "Binary Search",
    "description": "Search sorted array in O(log n) time",
    "example_inputs": [
      {
        "name": "Basic Search - Target Found",
        "input": {"array": [1, 3, 5, 7, 9], "target": 5}
      }
    ],
    "input_schema": {...}
  }
]
```

---

#### `POST /api/trace/unified`

**Purpose:** Generate trace for any registered algorithm.

**Request:**

```json
{
  "algorithm": "binary-search",
  "input": {
    "array": [1, 3, 5, 7, 9, 11, 13, 15],
    "target": 7
  }
}
```

**Response:**

```json
{
  "result": {
    "found": true,
    "index": 3,
    "comparisons": 3
  },
  "trace": {
    "steps": [
      {
        "step": 0,
        "type": "INITIAL_STATE",
        "timestamp": 0.001,
        "data": {
          "target": 7,
          "array_size": 8,
          "visualization": {
            "array": [
              {"index": 0, "value": 1, "state": "active_range"},
              ...
            ],
            "pointers": {"left": 0, "right": 7, "mid": null, "target": 7}
          }
        },
        "description": "üîç Searching for 7 in sorted array of 8 elements"
      }
    ],
    "total_steps": 12,
    "duration": 0.023
  },
  "metadata": {
    "algorithm": "binary-search",
    "display_name": "Binary Search",           # REQUIRED
    "visualization_type": "array",             # REQUIRED
    "visualization_config": {...},
    "prediction_points": [...],
    "input_size": 8
  }
}
```

**Error Response (400):**

```json
{
  "error": "Unknown algorithm: 'merge-sort'",
  "available_algorithms": ["binary-search", "interval-coverage"]
}
```

---

#### `GET /api/health`

**Purpose:** Health check with registry info.

**Response:**

```json
{
  "status": "healthy",
  "service": "algorithm-trace-backend",
  "algorithms_registered": 2,
  "available_algorithms": ["binary-search", "interval-coverage"]
}
```

---

## Adding a New Algorithm (CRITICAL WORKFLOW)

**Time Investment:** ~2 hours total (including FAA audit)

### Step 1: Implement AlgorithmTracer (30-45 min)

```python
# backend/algorithms/merge_sort.py
from typing import Any, List, Dict
from .base_tracer import AlgorithmTracer

class MergeSortTracer(AlgorithmTracer):
    def __init__(self):
        super().__init__()
        self.array = []

    def execute(self, input_data: Any) -> dict:
        # Validate input
        self.array = input_data.get('array', [])
        if not self.array:
            raise ValueError("Array cannot be empty")

        # CRITICAL: Set required metadata fields
        self.metadata = {
            'algorithm': 'merge-sort',
            'display_name': 'Merge Sort',              # ‚Üê REQUIRED
            'visualization_type': 'array',             # ‚Üê REQUIRED
            'visualization_config': {
                'element_renderer': 'number',
                'show_indices': True
            }
        }

        # Initial state
        self._add_step(
            "INITIAL_STATE",
            {'array': self.array.copy()},
            f"üî¢ Starting merge sort on array of {len(self.array)} elements"
        )

        # Run algorithm with trace generation
        sorted_array = self._merge_sort_recursive(self.array, 0, len(self.array) - 1)

        # Final state
        self._add_step(
            "ALGORITHM_COMPLETE",
            {'sorted_array': sorted_array},
            "‚úÖ Array sorted!"
        )

        # CRITICAL: Use _build_trace_result()
        return self._build_trace_result(sorted_array)

    def get_prediction_points(self) -> List[Dict[str, Any]]:
        """CRITICAL: Maximum 3 choices per question"""
        predictions = []
        for i, step in enumerate(self.trace):
            if step.type == "MERGE_DECISION":
                predictions.append({
                    'step_index': i,
                    'question': "Which element should be merged next?",
                    'choices': [  # ‚â§3 choices
                        {'id': 'left', 'label': 'Left subarray element'},
                        {'id': 'right', 'label': 'Right subarray element'}
                    ],
                    'correct_answer': 'left'
                })
        return predictions

    def generate_narrative(self, trace_result: dict) -> str:
        """Generate human-readable markdown narrative (REQUIRED v2.0+)"""
        narrative = "# Merge Sort Execution\n\n"

        # Input summary
        narrative += f"**Input Array:** {trace_result['result']}\n"
        narrative += f"**Array Size:** {len(trace_result['result'])}\n\n"

        # Step-by-step narrative
        for step in trace_result['trace']['steps']:
            narrative += f"## Step {step['step']}: {step['description']}\n\n"

            # Show visualization state with ALL relevant data
            if 'visualization' in step['data']:
                viz = step['data']['visualization']

                # Show current array state
                if 'array' in viz:
                    narrative += f"**Current Array:** {viz['array']}\n"

                # Show decision logic if applicable
                if step['type'] == 'MERGE_DECISION' and 'left_val' in step['data']:
                    left = step['data']['left_val']
                    right = step['data']['right_val']
                    narrative += f"**Comparison:** {left} vs {right}\n"
                    narrative += f"**Decision:** Select {min(left, right)} (smaller value)\n"

            narrative += "\n"

        # Final result
        narrative += "## Final Result\n\n"
        narrative += f"**Sorted Array:** {trace_result['result']}\n"
        narrative += f"**Total Steps:** {trace_result['trace']['total_steps']}\n"

        return narrative

    def _merge_sort_recursive(self, arr, left, right):
        # Implementation with _add_step() calls
        pass
```

---

### Step 2: Register in Registry (5 min)

```python
# backend/algorithms/registry.py

def register_algorithms():
    from .merge_sort import MergeSortTracer

    registry.register(
        name='merge-sort',                    # Unique ID (kebab-case)
        tracer_class=MergeSortTracer,
        display_name='Merge Sort',
        description='Divide-and-conquer sorting with O(n log n) complexity',
        example_inputs=[
            {
                'name': 'Basic Sort',
                'input': {'array': [5, 2, 8, 1, 9, 3]}
            }
        ]
    )
```

**That's it for backend!** No app.py changes needed. ‚ú®

---

### Step 3: Generate Narratives (10 min)

Run your algorithm on all example inputs and generate markdown narratives:

```bash
cd backend
python scripts/generate_narratives.py merge-sort
```

This creates files in `docs/narratives/merge-sort/`:

- `example_1_basic_sort.md`
- `example_2_large_array.md`
- etc.

---

### Step 3.5: FAA Audit (10-15 min) - NEW in v2.1

Run Forensic Arithmetic Audit on generated narratives:

1. Use `docs/compliance/FAA_PERSONA.md` as audit guide
2. Verify every quantitative claim with calculation
3. Check arithmetic correctness (not pedagogy)
4. Fix any errors and regenerate narratives
5. Repeat until FAA passes

**Critical:** This is a **BLOCKING gate**. No narrative proceeds with arithmetic errors.

**Common errors caught:**

- Copy-paste errors (same number after different operations)
- Stale state propagation (old values not updated)
- Visualization-text mismatches (text says 10, shows 8)
- Off-by-one errors in index calculations

**Expected time:**

- Initial audit: 10-15 minutes
- Re-audit after fixes: 5 minutes
- Total for clean narrative: ~15 minutes
- Total for narrative with errors: ~35 minutes (including fixes)

---

### Step 4: Backend Compliance Checklist (10 min)

Complete `docs/compliance/BACKEND_CHECKLIST.md`:

**Critical Checks:**

- [ ] Metadata has `algorithm`, `display_name`, `visualization_type`
- [ ] Trace structure matches contract
- [ ] Visualization state uses `state` (string), not `visual_state` (dict)
- [ ] Prediction points have ‚â§3 choices
- [ ] Inherits from `AlgorithmTracer`
- [ ] Uses `_add_step()` and `_build_trace_result()`
- [ ] **Implements `generate_narrative()` method** (v2.0+)
- [ ] **Narratives pass FAA arithmetic audit** (v2.1)

**Rule:** If >3 items fail, stop and fix before proceeding.

---

### Step 5: QA Narrative Review (15 min)

QA reviews FAA-approved narratives for:

- Logical completeness
- Temporal coherence
- Decision transparency
- **Assumes arithmetic already verified by FAA**

---

### Step 6: Create/Reuse Visualization (0-30 min)

**Option A: Reuse (0 min)** - Recommended for array-based algorithms

```python
self.metadata = {
    'visualization_type': 'array',  # Reuses ArrayView automatically
}
```

**Option B: New Component (30 min)** - For custom visualizations

```javascript
// frontend/src/components/visualizations/GraphView.jsx
const GraphView = ({ step, config = {} }) => {
  const visualization = step?.data?.visualization;

  return (
    // CRITICAL: Use items-start + mx-auto pattern
    <div className="h-full flex flex-col items-start overflow-auto py-4 px-6">
      <div className="mx-auto">{/* Your visualization */}</div>
    </div>
  );
};
```

---

### Step 7: Register Visualization (5 min, if new)

```javascript
// frontend/src/utils/visualizationRegistry.js
import GraphView from "../components/visualizations/GraphView";

const VISUALIZATION_REGISTRY = {
  array: ArrayView,
  timeline: TimelineView,
  graph: GraphView, // ‚Üê Add new component
};
```

---

### Step 8: Frontend & QA Checklists (15 min)

**Frontend Checklist (`docs/compliance/FRONTEND_CHECKLIST.md`):**

- [ ] Overflow pattern: `items-start` + `mx-auto` (NOT `items-center`)
- [ ] Modal IDs: `#prediction-modal`, `#completion-modal`
- [ ] Keyboard shortcuts work
- [ ] Component receives `step` and `config` props

**QA Checklist (`docs/compliance/QA_INTEGRATION_CHECKLIST.md`):**

- [ ] All 14 test suites pass
- [ ] No regressions in existing algorithms
- [ ] Overflow testing with 20+ elements

---

## Component Architecture

### State Management (Context API)

The application uses React Context to manage state, avoiding prop drilling and "God Object" patterns.

- **`TraceContext`** - Raw trace data and metadata loading
- **`NavigationContext`** - Current step index and derived step data
- **`PredictionContext`** - Active learning mode and scoring logic
- **`KeyboardContext`** - Centralized event handling with priority system

### Context Consumers (Hooks)

Custom hooks now serve as convenient wrappers around Contexts:

- **`useTraceLoader`** - Consumes `TraceContext`
- **`useTraceNavigation`** - Consumes `NavigationContext`
- **`usePredictionMode`** - Consumes `PredictionContext`
- **`useKeyboardShortcuts`** - Consumes `KeyboardContext`

### UI Components

- **`AlgorithmSwitcher`** - Algorithm dropdown selector
- **`ControlBar`** - Navigation buttons
- **`PredictionModal`** - Interactive prediction prompts (ID: `#prediction-modal`)
- **`CompletionModal`** - Success screen with stats (ID: `#completion-modal`)
- **`KeyboardHints`** - Shortcut guide
- **Visualizations** (LEFT panel): `ArrayView`, `TimelineView`
- **Algorithm States** (RIGHT panel): `BinarySearchState`, `IntervalCoverageState`

---

## Prediction Mode (Active Learning)

**Transform passive observation into active engagement.**

### How It Works

1. Algorithm identifies decision points via `get_prediction_points()`
2. Frontend pauses at these points
3. Student predicts outcome before seeing answer
4. Immediate feedback with accuracy tracking

### Example: Binary Search

```python
def get_prediction_points(self):
    predictions = []
    for i, step in enumerate(self.trace):
        if step.type == "CALCULATE_MID":
            predictions.append({
                'step_index': i,
                'question': f"Compare mid ({mid}) with target ({target}). What's next?",
                'choices': [  # ‚â§3 choices (CONSTRAINED)
                    {'id': 'found', 'label': 'Found!'},
                    {'id': 'search-left', 'label': 'Search Left'},
                    {'id': 'search-right', 'label': 'Search Right'}
                ],
                'correct_answer': 'search-right'
            })
    return predictions
```

### Keyboard Shortcuts (LOCKED)

| Keys           | Action                | Context             |
| -------------- | --------------------- | ------------------- |
| `‚Üí` or `Space` | Next step             | During navigation   |
| `‚Üê`            | Previous step         | During navigation   |
| `R` or `Home`  | Reset to start        | Anytime             |
| `End`          | Jump to end           | During navigation   |
| `K`            | Predict first option  | In prediction modal |
| `C`            | Predict second option | In prediction modal |
| `S`            | Skip question         | In prediction modal |
| `Enter`        | Submit answer         | In prediction modal |
| `Esc`          | Close modal           | In completion modal |

### Accuracy Feedback Tiers

- **90-100%**: "üéâ Excellent! You've mastered this algorithm!"
- **70-89%**: "üëç Great job! You have a solid understanding."
- **50-69%**: "üìö Good effort! Review the patterns for better accuracy."
- **<50%**: "üí™ Keep practicing! Focus on understanding each decision."

---

## Environment Configuration

### Backend

```bash
# Required
FLASK_ENV=production
CORS_ORIGINS=https://your-frontend-domain.com

# Optional
MAX_INTERVALS=100
MAX_STEPS=10000
```

### Frontend

```bash
# .env.development
REACT_APP_API_URL=http://localhost:5000/api

# .env.production
REACT_APP_API_URL=https://api.your-domain.com/api
```

---

## Quick Start

### Backend

```bash
cd backend
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
python app.py
```

**Expected Output:**

```
üöÄ Algorithm Trace Backend Starting...
üåê Running on: http://localhost:5000
üìä Registered Algorithms: 2
   - interval-coverage: Interval Coverage
   - binary-search: Binary Search
```

### Frontend

```bash
cd frontend
pnpm install  # or: npm install
pnpm start    # or: npm start
```

Frontend runs on `http://localhost:3000`

---

## Testing

### Backend Testing

```bash
# Discovery endpoint
curl http://localhost:5000/api/algorithms | jq

# Unified endpoint - Binary Search
curl -X POST http://localhost:5000/api/trace/unified \
  -H "Content-Type: application/json" \
  -d '{"algorithm": "binary-search", "input": {"array": [1,3,5,7,9], "target": 5}}' | jq

# Error handling - Unknown algorithm
curl -X POST http://localhost:5000/api/trace/unified \
  -H "Content-Type: application/json" \
  -d '{"algorithm": "unknown", "input": {}}' | jq
```

### Frontend Testing (Manual)

**Critical Tests:**

1. **Algorithm Discovery** - Dropdown shows all algorithms
2. **Visualization Types** - ArrayView (Binary Search), TimelineView (Interval Coverage)
3. **Overflow Handling** - Test with 20+ elements, verify no left-side cutoff
4. **Prediction Mode** - Enable, make predictions, check accuracy tracking
5. **Keyboard Shortcuts** - Test ‚Üê‚Üí navigation, R reset, K/C/S prediction
6. **Modal IDs** - Verify `#prediction-modal`, `#completion-modal` exist
7. **Responsive** - Test 3 viewport sizes (desktop, tablet, mobile)

---

## Current Algorithms

| Algorithm             | Visualization | Status  | Prediction Points        |
| --------------------- | ------------- | ------- | ------------------------ |
| **Interval Coverage** | Timeline      | ‚úÖ Live | Keep/Covered decisions   |
| **Binary Search**     | Array         | ‚úÖ Live | Search direction choices |

---

## Deployment

### Backend (Production)

```bash
pip install gunicorn
gunicorn -w 4 -b 0.0.0.0:5000 app:app
```

**Environment Variables:**

- `FLASK_ENV=production`
- `CORS_ORIGINS=https://your-frontend-domain.com`

### Frontend (Production)

```bash
pnpm run build  # Output: ./build/
```

**Deployment Options:** Vercel, Netlify, AWS S3+CloudFront, GitHub Pages

**Required:** `REACT_APP_API_URL=https://api.your-domain.com/api`

---

## Support

- **GitHub Issues:** Open with [Bug], [Feature], or [Question] tag
- **Documentation:**
  - `docs/compliance/WORKFLOW.md` - Single source of truth for workflow & architecture
  - `docs/compliance/` - Compliance checklists (Backend, FAA, Frontend, QA)
  - `docs/ADR/` - Architecture Decision Records

---

## License

MIT License

---

**Status:** ‚úÖ Platform Architecture Complete - Ready for Algorithm Expansion

**Next Steps:** Add 3rd algorithm to validate scalability

---
